## üí° Tips for Obtaining Force Jacobians

### üéì For Scholars

**Hindsight (Aug 9 2025)**: After publication, we learned that the core insights of the eigensystem analysis presented here were previously developed by Poya et al.
(2023) in their comprehensive work. Their paper provides an extensive theoretical foundation for closed-form tangent eigensystems in principal stretches, including the eigendecomposition techniques that form the basis of our implementation.

We acknowledge that many of the fundamental mathematical insights we independently arrived at using a different approach were already established in their work.
We do not claim the underlying analysis as our contribution, but we believe this presentation remains valuable for practitioners. For instance, our treatment includes shell elements with 3√ó2 deformation gradients, extending beyond the 2√ó2 case discussed in their work. Although the conversion may be straightforward for experts, having explicit formulations readily available can benefit implementation efforts.

Readers who wish to mention this article should refer to the original work:

```bibtex
@article{poya2023variational,
  author = {Poya, Roman and Ortigosa, Rogelio and Gil, Antonio J.},
  title = {Variational schemes and mixed finite elements for large strain isotropic elasticity in principal stretches: Closed-form tangent eigensystems, convexity conditions, and stabilised elasticity},
  journal = {International Journal for Numerical Methods in Engineering},
  volume = {124},
  number = {16},
  pages = {3436-3493},
  keywords = {convexity conditions, large strain elasticity, mixed finite elements, principal stretches},
  doi = {https://doi.org/10.1002/nme.7254},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.7254},
  year = {2023}
}
```

For additional implementation details and the practical application of these techniques, readers may also refer to our supplementary materials in Section E and F.9 of [(Link)](https://drive.google.com/file/d/1ptjFNVufPBV4-vb5UDh1yTgz8-esjaSF/view).

## üèÅ The Final Results

Since the materials below are lengthy, we provide a summary of the final results. If you're excited, go ahead and proceed reading:

- Any eigen-filtered force jacobian of isotropic energies expressed purely in terms of their singular values can all be computed in closed form, provided that $\frac{\partial \Psi}{\partial \sigma}$ and $\frac{\partial^2 \Psi}{\partial \sigma^2}$ are symbolically known.

- If the energy is expressed in terms of invariants, eigen-filtered force jacobian can be obtained in closed form by using only $\frac{\partial \Psi}{\partial I_k}$ and $\frac{\partial^2 \Psi}{\partial I_k^2}$, where $I_k$ are the Cauchy-Green invariants or those from Smith et al. [(1)](#1).

- Our analysis can be used to re-derive exactly the same system from Smith et al. [(1)](#1).

## üìñ What's the Story?

If you are solving it implicitly, you‚Äôll probably need both the force and the force jacobian of a potential energy ‚ö° with respect to üé¢ strain.
For example, let $\Psi(F)$ denote a hyper-elasticity potential energy with respect to a strain tensor $F$.
$F$ is a $3 \times 3$ matrix for solids and a $3 \times 2$ matrix for shells. It is alternatively called the deformation gradient.
Force derivatives refer to $\frac{\partial^2 \Psi}{\partial f_i \partial f_j}$, where **$f_i$ is a single entry of matrix $F$**.

## üöß What's the Problem?

We could manage üòñ to calculate the force $\frac{\partial \Psi}{\partial f_i}$, but the force jacobian $\frac{\partial^2 \Psi}{\partial f_i \partial f_j}$ is so hard üòµ‚Äçüí´ that it drives us crazy! ü§Ø
But what a bummer! üò∞ The reality is **cruel üíÄ**; what we actually need is the üîç **eigen-decomposed force jacobian**, not simply the force jacobian! üò≠
They are necessary to project them onto the semi-positive space, ensuring that the Newton's method does descend the energy to be minimized. ü§î
A brute-force solution using numerical factorization is too üêå slow (it's a dense $9 \times 9$ matrix!).
Symbolic math software üí• explodes.
üöß Dead end üöß.

> [!NOTE]
> Some readers may instead opt for position-based dynamics or projective dynamics, which do not require force jacobian.
> However, these first-order convergent methods are not suitable for highly stiff materials or high-resolution meshes due to their slow convergence üê¢.
> If you‚Äôre skeptical, try it yourself.
> They are best suited for real-time applications, such as üéÆ video games.

## ü§î Why Not Just Use Past Literature?

We know that there is excellent literature out there üìö but it has ‚õî limitations.
The most popular one [(1)](#1) heavily depends on invariants, but not all isotropic energies can be clearly expressed with them, or at least not without considerable human effort.
For example, how can we write a flavor of [Ogden](https://en.wikipedia.org/wiki/Ogden_hyperelastic_model) energy $\sum_k \left( \lambda_1^{0.5^k} + \lambda_2^{0.5^k} + \lambda_3^{0.5^k} - 3 \right)$ using invariants?

> [!NOTE]
> A practical example where our singular-value eigenanalysis is essential is the isotropic strain-limiting energies, which are discussed in the supplementary.

Other method [(2)](#2) can handle such cases but have singularities when two stretches are the same along principal axes.
This means that even a rest pose üõü is considered a singular case.
Technically, our method is mathematically a variant of Zhu [(3)](#3); though, we eliminate (not all) the singularities they have and offer a more explicit solution.
A notable difference is that ours has the ability to re-derive the eigen system revealed by Smith et al. [(1)](#1).
This is only possible with our formulation, not with Zhu [(3)](#3).

<a id="1">[1]</a> Breannan Smith, Fernando De Goes, and Theodore Kim. 2019. Analytic Eigensystems for Isotropic Distortion Energies. ACM Trans. Graph. <https://doi.org/10.1145/3241041>

<a id="2">[2]</a> Alexey Stomakhin, Russell Howes, Craig Schroeder, and Joseph M. Teran. 2012. Energetically consistent invertible elasticity. In Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA '12).

<a id="3">[3]</a> Yufeng Zhu. 2021. Eigen Space of Mesh Distortion Energy Hessian. <https://arxiv.org/abs/2103.08141>

## üéØ Our Goals

Without further ado, let us pin üìå our objectives:

- We just need $\frac{\partial \Psi}{\partial f_i}$ and the eigen-filtered $\frac{\partial^2 \Psi}{\partial f_i \partial f_j}$; nothing else.
- We only use singular values, not invariants.
- It has to be both reasonably fast üöÄ and simple ‚úèÔ∏è.

> [!TIP]
> Once you obtain $\frac{\partial \Psi}{\partial f_i}$ and $\frac{\partial^2 \Psi}{\partial f_i \partial f_j}$; we can apply the following chain rule
>
> ```math
> \begin{align}
>   \frac{\partial \Psi}{\partial x} = \sum_j \frac{\partial f_j }{\partial x} \frac{\partial \Psi}{\partial f_j}, \nonumber \\
>   \frac{\partial^2 \Psi}{\partial x^2} = \sum_{i,j} \frac{\partial^2 \Psi}{\partial f_i \partial f_j} \left(\frac{\partial f_i}{\partial x}\right)\left(\frac{\partial f_j}{\partial x}\right)^T, \nonumber
> \end{align}
> ```
>
> to convert them to $\frac{\partial \Psi}{\partial x}$ and $\frac{\partial^2 \Psi}{\partial x^2}$, where $x$ is a element vertex of interest.
> Once converted, both can be directly plugged into the Newton's method.
> Just to be sure; the second chain rule only holds when $\frac{\partial^2 f_i}{\partial x^2} = 0$.
> For linear elements, $F$ is linear in $x$. $\frac{\partial f_i}{\partial x}$ is a simple matter to compute, if you know how to get $F$ from $x$.

## üîç Eigen-decomposed Force Jacobians

If you are only interested in **isotropic** squishables üéà (e.g., their stretches aren't biased in any direction), we have a good solution üéÅ for it.

### ü™µ Volumetric Elasticity Energies

For 3D elasticity strain $F_{\mathrm{3D}}$ is a $3 \times 3$ matrix. First we realize that isotropic distortion energies are written as a function of singular values of $F$ such that $\Psi_{\mathrm{3D}}(\sigma_1,\sigma_2,\sigma_3)$, where $\sigma_{1 \cdots 3}$ are the singular values of $F$.
Let us list some popular ones (with Lam√© and model-specific parameters substituted):

| Model | $\Psi_{\mathrm{3D}}(\sigma_1,\sigma_2,\sigma_3)$ |
|-------|------------|
| **ARAP** | $\sum_{j=1}^{3} (\sigma_j - 1)^2$ |
| **Symmetric Dirichlet** | $\sum_{j=1}^{3} \left(\sigma_j^2 + \frac{1}{\sigma_j^2}\right)$ |
| **MIPS** | $\sum_{j=1}^{3} \frac{\sigma_j^2}{\sigma_1 \sigma_2 \sigma_3}$ |
| **Ogden** | $\sum_{k=0}^{4} \left(\sigma_1^{0.5^k} + \sigma_2^{0.5^k} + \sigma_3^{0.5^k} - 3\right)$ |
| **Yeoh** | $\sum_{k=1}^{3} \left(\sigma_1^2 + \sigma_2^2 + \sigma_3^2 - 3\right)^{k}$ |

$\sigma_{1 \cdots 3}$ are obtained via a numerical SVD of $F$ at a reasonable time.
It's just a $3 \times 3$ matrix.
You may find that our closed-formed SVD routine (Apache v2.0) below is useful.

```
/* Apache v2.0 License */
template <unsigned R, unsigned C>
Svd<R, C> svd(const Mat<R, C> &F) {
    Mat<C, C> V;
    Vec<C> lambda;
    solve_symm_eigen<C>(F.transpose() * F, lambda, V);
    for (int i = 0; i < C; ++i) {
        lambda[i] = sqrtf(fmax(0.0f,lambda[i]));
    }
    Mat<R, C> U = F * V;
    for (int i = 0; i < U.cols(); i++) {
        U.col(i).normalize();
    }
    /* Do an inversion-aware flip if needed */
    return {U, lambda, V.transpose()};
}
```

This code can also handle non-square $F$, which is helpful for shell elements that we will cover later.
At the line ```solve_symm_eigen()``` we can use a closed-form solution.

> [!NOTE]
> You may argue that the eigen decomposition of symmetric $3 \times 3$ matrices is expensive.
> Yes, an iterative method like the [Lanczos algorithm](https://en.wikipedia.org/wiki/Lanczos_algorithm) is expensive.
> But for such a small matrix size, a direct solver is possible.
> It may sound overwhelming, but aside from finding the three roots of a cubic equation, everything else is trivial.
> I've written our own closed-form $3 \times 3$ eigen decomposition [(Code Link)](../eigsys/eig-hpp), so go ahead and use it!

Let's get our hands dirty üëê. We‚Äôll first write down a set of matrices and scalars.

```math
\begin{align}
    Q_1 = \frac{1}{\sqrt{2}} U_{3 \times 3} \begin{bmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 0
    \end{bmatrix} V_{3 \times 3}^T, \hspace{2mm}
    Q_2 = \frac{1}{\sqrt{2}} U_{3 \times 3} \begin{bmatrix}
        0 & 0 & 1 \\
        0 & 0 & 0 \\
        -1 & 0 & 0
    \end{bmatrix} V_{3 \times 3}^T, \nonumber \\
    Q_3 = \frac{1}{\sqrt{2}} U_{3 \times 3} \begin{bmatrix}
        0 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & -1 & 0
    \end{bmatrix} V_{3 \times 3}^T, \hspace{2mm}
    Q_4 = \frac{1}{\sqrt{2}} U_{3 \times 3} \begin{bmatrix}
        0 & 1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 0
    \end{bmatrix} V_{3 \times 3}^T, \nonumber \\
    Q_5 = \frac{1}{\sqrt{2}} U_{3 \times 3} \begin{bmatrix}
        0 & 0 & 1 \\
        0 & 0 & 0 \\
        1 & 0 & 0
    \end{bmatrix} V_{3 \times 3}^T, \hspace{2mm}
    Q_6 = \frac{1}{\sqrt{2}} U_{3 \times 3} \begin{bmatrix}
        0 & 0 & 0 \\
        0 & 0 & 1 \\
        0 & 1 & 0
    \end{bmatrix} V_{3 \times 3}^T. \nonumber
\end{align}
```

$U_{3 \times 3}$ and $V_{3 \times 3}^T$ are rotation matrices obtained by the above SVD call of $F$.
Next, write down the following $3 \times 3$ symmetric matrix:

```math
\begin{equation}
    H_{3 \times 3} = \begin{bmatrix}
    \displaystyle \frac{\partial^2}{\partial \sigma_1^2} & \displaystyle \frac{\partial^2}{\partial \sigma_1 \partial \sigma_2} & \displaystyle \frac{\partial^2}{\partial \sigma_1 \partial \sigma_3} \\
     & \displaystyle \frac{\partial^2}{\partial \sigma_2^2} & \displaystyle \frac{\partial^2}{\partial \sigma_2 \partial \sigma_3} \\
    \mathrm{Sym} & & \displaystyle \frac{\partial^2}{\partial \sigma_3^2}
    \end{bmatrix}\Psi_{\mathrm{3D}}. \nonumber
\end{equation}
```

Let $w_1$, $w_2$, and $w_3$ be a set of eigenvectors of $H_{3 \times 3}$, and $\beta_1$, $\beta_2$, and $\beta_3$ be the corresponding eigenvalues.
We can use a $3 \times 3$ numerical eigen solver to get them.
For a simple case where $H_{3 \times 3}$ is diagonal, such as in ARAP, this decomposition is not necessary.
Let us write down a few more things:

```math
\begin{align}
    \lambda_1 = \frac{1}{\sigma_1 + \sigma_2}\left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_1}+\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_2}\right), \hspace{3mm}
    \lambda_2 = \frac{1}{\sigma_1 + \sigma_3}\left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_1}+\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_3}\right), \nonumber \\
    \lambda_3 = \frac{1}{\sigma_2 + \sigma_3}\left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_2}+\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_3}\right), \hspace{3mm}
    \lambda_4 = \frac{1}{\sigma_1 - \sigma_2}\left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_1}-\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_2}\right), \nonumber \\
    \lambda_5 = \frac{1}{\sigma_1 - \sigma_3}\left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_1}-\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_3}\right), \hspace{3mm}
    \lambda_6 = \frac{1}{\sigma_2 - \sigma_3}\left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_2}-\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_3}\right), \nonumber 
\end{align}
```

```math
\begin{align}
    Q_7 = U_{3 \times 3} \textrm{diag}\left(w_1\right) V_{3 \times 3}^T, \nonumber \\
    Q_8 = U_{3 \times 3} \textrm{diag}\left(w_2\right) V_{3 \times 3}^T, \nonumber \\
    Q_9 = U_{3 \times 3} \textrm{diag}\left(w_3\right) V_{3 \times 3}^T, \nonumber \\
    \lambda_7 = \beta_1, \hspace{2mm}
    \lambda_8 = \beta_2, \hspace{2mm}
    \lambda_9 = \beta_3. \nonumber
\end{align}
```

All right, we have everything we need.
Make sure that we have 9 matrices $Q_{1 \cdots 9}$ and their corresponding scalars $\lambda_{1 \cdots 9}$ ready.

> [!NOTE]
> Some readers may notice that our results are consistent with [(1)](#1) in that the first six eigenmatrices are static, while the remaining three depend on the deformation. Indeed, our technique is closely related to [(1)](#1).
> $Q_{1 \cdots 3}$ are called twist modes, $Q_{4 \cdots 6}$ are called flip modes, and $Q_{7 \cdots 9}$ are called scaling modes.

Now we present results.
The first derivative (the force) is

```math
\begin{align}
    \frac{\partial \Psi_{\mathrm{3D}}}{\partial f_i} = \sum_k^3 \frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_k} \left( u_k \cdot \left( S_i v_k \right)\right), \nonumber
\end{align}
```

where $S_i$ is a $3 \times 3$ stencil matrix with a single entry of one at the same row and column as $f_i$, and zeros elsewhere.
Just to be clear, $F$ is a $3 \times 3$ matrix and thus has 9 entries. Therefore, $f_i$ loops from 1 to 9.
The loop order in the matrix view does not matter; even a random order will work.
$u_k$ and $v_k$ are vectors representing the $k$-th columns of the rotation matrices $U_{3 \times 3}$ and $V_{3 \times 3}$, respectively.
Finally, the eigen-filtered force jacobian are

```math
\begin{equation}
    \frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial f_i \partial f_j} = \sum_k^9 \max(\lambda_k,0) q_i q_j, \nonumber
\end{equation}
```

where $q_k$ is a single entry of $Q_k$ located at the same row and column as $f_k$.
Yes, it‚Äôs really that simple.
You don't believe it?
We've writen a Python code to numerically check its correctness. Check this out [(Code)](../eigsys/eigsys_3.py).

Okay, there's one more step.
When $\sigma_i = \sigma_j$, $\lambda_{4,5,6}$ result in division by zero.

> [!NOTE]
> Of course, if you use a symbolic package to simplify the symbolic expressions, singularities like $\sigma_i = \sigma_j$ may be removed. However, wouldn‚Äôt it be better if we could eliminate these singularities while keeping $\Psi_{\mathrm{3D}}$ abstracted?
> This way, you won‚Äôt have to manually translate the symbolic expressions into code.

But in this case, we can use the following approximation instead:

```math
\begin{equation}
   \frac{1}{\sigma_i - \sigma_j} \left( \frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_i} - \frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_j} \right) \approx \frac{1}{2}\left(\frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial \sigma_i^2} + \frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial \sigma_j^2}\right) - \frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial \sigma_i \partial \sigma_j}. \nonumber
\end{equation}
```

> [!NOTE]
> For this approximation to work, the energy definition must satisfy the Valanis-Landel hypothesis: $\Psi_{\mathrm{3D}}(\sigma_1, \sigma_2, \sigma_3)$ should be invariant to the order of $\sigma_1$, $\sigma_2$, and $\sigma_3$. For example, $\Psi_{\mathrm{3D}}(\sigma_1, \sigma_2, \sigma_3) = \Psi_{\mathrm{3D}}(\sigma_2, \sigma_1, \sigma_3)$.
> All isotropic energies must satisfy this condition.

Our Python code üêç above also checks this, and we assure you that it works even $\sigma_i = \sigma_j$ exactly! ü•≥
If you‚Äôre feeling even lazy ü•± to write symbolic expressions for $\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma_i}$ and $\frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial \sigma_i \partial \sigma_j}$, you can let a smart automatic differentiator do it for you.
The overhead should be negligible since the variables are only  $\sigma_1$, $\sigma_2$ and $\sigma_3$, and major energies are concisely written in terms of singular values.
This way, we can truly automate ü§ñ everything, meaning we won‚Äôt need to work on any math üòô whenever we change the target elastic energy.
Plug and play. üîå

### üß≠ Using Cauchy-Green Invariants

Picky readers ü§è point out that singularities still exist $\sigma_i + \sigma_j = 0$, which is a bit extreme since elements need to be inverted to reach this condition, assuming you opt an inversion-aware SVD.
Of course, this occurs frequently in graphics applications, so it should be taken seriously.
The good news üì∞ is that we can remove all the singularities in cases where the energies are expressed as a function of Cauchy-Green invariants, as shown by Stomakhin et al. [(2)](#2).
More specifically, this means that the potential is defined as:

```math
\begin{equation}
   \Psi_{\mathrm{3D}} = \Psi(I_1,I_2,I_3). \nonumber
 \end{equation}
 ```

Here, $\Psi_{\mathrm{3D}}$ can be quite complex, even something weird like $\Psi_{\mathrm{3D}} = \frac{(I_1 + I_2 - 6)^2}{I_1} + \sqrt{\frac{I_3}{I_1} + 1} - 2$, is acceptable.
$I_{1 \cdots 3}$ are Cauchy-Green invariants:

```math
\begin{align}
   I_1 &= \mathrm{tr}\left(F^TF\right) = \sigma_1^2 + \sigma_2^2 + \sigma_3^3, \nonumber \\
   I_2 &= \frac{1}{2}\left(\left(I_1\right)^2 - \|F^TF\|^2\right) = (\sigma_1 \sigma_2)^2 + (\sigma_2 \sigma_3)^2 + (\sigma_3 \sigma_1)^2, \nonumber \\
   I_3 &= \mathrm{det}\left(F^TF\right) = (\sigma_1 \sigma_2 \sigma_3)^2. \nonumber
\end{align}
```

With the above definitions, all singularies are cleanly removed such that:

```math
\begin{equation}
  \begin{bmatrix}
      \lambda_1 \\
      \lambda_2 \\
      \lambda_3 \\
      \lambda_4 \\
      \lambda_5 \\
      \lambda_6
  \end{bmatrix} =
  2 \begin{bmatrix}
     1 & \sigma_1 \sigma_2 + \sigma_3^2 & \sigma_1 \sigma_2 \sigma_3^2 \\
     1 & \sigma_1 \sigma_3 + \sigma_2^2 & \sigma_1 \sigma_2^2 \sigma_3 \\
     1 & \sigma_2 \sigma_3 + \sigma_1^2 & \sigma_1^2 \sigma_2 \sigma_3 \\
     1 & \sigma_3^2 - \sigma_1 \sigma_2 & -\sigma_1 \sigma_2 \sigma_3^2 \\
     1 & \sigma_2^2 - \sigma_1 \sigma_3 & -\sigma_1 \sigma_2^2 \sigma_3 \\
     1 & \sigma_1^2 - \sigma_2 \sigma_3 & -\sigma_1^2 \sigma_2 \sigma_3
  \end{bmatrix}
  \begin{bmatrix}
      \frac{\partial}{\partial I_1} \\
      \frac{\partial}{\partial I_2} \\
      \frac{\partial}{\partial I_3}
  \end{bmatrix}
  \Psi_{\mathrm{3D}}. \nonumber
\end{equation}
```

> [!NOTE]
> Of course, if $\frac{\partial \Psi_{\mathrm{3D}}}{\partial I_i}$ includes singularities, we can't do anything about them, but that's not our fault.

And there's more: if you can describe your target energy using invariants, you can **also** express $H_{3 \times 3}$ without directly differentiating $\Psi_{\mathrm{3D}}$ with respect to the singular values:

```math
H_{3 \times 3} = \sum_{i,j} \frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial I_i \partial I_j} \left(\frac{\partial I_i}{\partial \sigma}\right)\left(\frac{\partial I_j}{\partial \sigma}\right)^T + \sum_k \left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial I_k}\right) \left(\frac{\partial^2 I_k}{\partial \sigma^2}\right),
```

where

```math
\frac{\partial I_i}{\partial \sigma} = \begin{bmatrix}
  \displaystyle \frac{\partial}{\partial \sigma_1} \\
  \displaystyle \frac{\partial}{\partial \sigma_2} \\
  \displaystyle \frac{\partial}{\partial \sigma_3}
\end{bmatrix} I_i, \hspace{3mm}
\frac{\partial^2 I_k}{\partial \sigma^2} = \begin{bmatrix}
  \displaystyle \frac{\partial^2}{\partial \sigma_1^2} & \displaystyle \frac{\partial^2}{\partial \sigma_1 \partial \sigma_2} & \displaystyle \frac{\partial^2}{\partial \sigma_1 \partial \sigma_3} \\
 & \displaystyle \frac{\partial^2}{\partial \sigma_2^2} & \displaystyle \frac{\partial^2}{\partial \sigma_2 \partial \sigma_3} \\
 \mathrm{Sym} &  & \displaystyle \frac{\partial^2}{\partial \sigma_3^2}
\end{bmatrix} I_k.
```

To make your life easier üòô, we'll write down explicit expressions:

```math
\begin{align}
\frac{\partial I_1}{\partial \sigma} = 2 \left[\begin{matrix} \sigma_{1}\\ \sigma_{2}\\ \sigma_{3}\end{matrix}\right], \hspace{3mm} \frac{\partial I_2}{\partial \sigma} = 2 \left[\begin{matrix}\sigma_{1} \sigma_{2}^{2} + \sigma_{1} \sigma_{3}^{2}\\\sigma_{1}^{2} \sigma_{2} + \sigma_{2} \sigma_{3}^{2}\\\sigma_{1}^{2} \sigma_{3} + \sigma_{2}^{2} \sigma_{3}\end{matrix}\right], \hspace{3mm} \frac{\partial I_3}{\partial \sigma} = 2 \left[\begin{matrix}\sigma_{1} \sigma_{2}^{2} \sigma_{3}^{2}\\\sigma_{1}^{2} \sigma_{2} \sigma_{3}^{2}\\\sigma_{1}^{2} \sigma_{2}^{2} \sigma_{3}\end{matrix}\right]. \nonumber
\end{align} 
```

```math
\begin{align}
\frac{\partial^2 I_1}{\partial \sigma^2} = 2 \left[\begin{matrix}1 & 0 & 0\\0 & 1 & 0\\0 & 0 & 1\end{matrix}\right], \hspace{2mm} \frac{\partial^2 I_2}{\partial \sigma^2} = 2 \left[\begin{matrix}\sigma_{2}^{2} + \sigma_{3}^{2} & 2 \sigma_{1} \sigma_{2} & 2 \sigma_{1} \sigma_{3}\\ & \sigma_{1}^{2} + \sigma_{3}^{2} & 2 \sigma_{2} \sigma_{3}\\ \mathrm{Sym} & & \sigma_{1}^{2} + \sigma_{2}^{2}\end{matrix}\right], \nonumber \\
\frac{\partial^2 I_3}{\partial \sigma^2} = 2 \left[\begin{matrix}\sigma_{2}^{2} \sigma_{3}^{2} & 2 \sigma_{1} \sigma_{2} \sigma_{3}^{2} & 2 \sigma_{1} \sigma_{2}^{2} \sigma_{3}\\ & \sigma_{1}^{2} \sigma_{3}^{2} & 2 \sigma_{1}^{2} \sigma_{2} \sigma_{3}\\ \mathrm{Sym} &  & \sigma_{1}^{2} \sigma_{2}^{2}\end{matrix}\right]. \nonumber
\end{align}
```

The first derivative of $\Psi_{\mathrm{3D}}$ can be **also** computed without $\frac{\partial \Psi_{\mathrm{3D}}}{\partial \sigma}$ using the chain rule:

```math
\frac{\partial \Psi_{\mathrm{3D}}}{\partial f_k} = \sum_k \left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial I_k}\right) \frac{\partial I_k}{\partial f_i},
```

where $C = F^T F$ and

```math
\begin{align}
  \frac{\partial I_1}{\partial F} = 2 F, \hspace{3mm}
  \frac{\partial I_2}{\partial F} = 2 ( \mathrm{tr}\left(C\right) F - F C ), \hspace{3mm}
  \frac{\partial I_3}{\partial F} = 2 F \mathrm{adj} \left(C\right)^T.
\nonumber
\end{align}
```

Just to be clear, $\frac{\partial I_k}{\partial F}$ yields a $3 \times 3$ matrix, and $\frac{\partial I_k}{\partial f_i}$ represents the single entry of $\frac{\partial I_k}{\partial F}$ corresponding to the same row and column as $f_i$.

This way, you will only need to prepare $\frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial I_i \partial I_j}$ and $\frac{\partial \Psi_{\mathrm{3D}}}{\partial I_k}$ whenever you change üîÅ the target energy density.
This approach is often more convenient than working directly with the singular values because many (not all) major energy ‚ö° functions are described in terms of Cauchy-Green invariants, and it's also more succinct.
Everything else works without changing your code ü•≥.
We wrote a Python script to verify the correctness of the materials in this section [(Code)](../eigsys/eigsys_invariants_3.py).

### ü•º For Shell Elasticity

We can extend the idea above for triangular meshes.
Recalling that $F$ is $3 \times 2$ for shells, the dimensions of $U$ and $V^T$ change to $U_{3 \times 2}$ and $V^T_{2 \times 2}$, respectively.
If you use our SVD code above, such changes are automatically handled.
How to obrain $F$ from $x$ is mentioned in the paper supplementary.

With this setting, the list of matrices $Q$ and scalars $\lambda$ are given by

```math
\begin{align}
    Q_1 = \frac{1}{\sqrt{2}} U_{3 \times 3}\begin{bmatrix} 0 & 1 \\ -1 & 0 \\ 0 & 0\end{bmatrix} V_{2 \times 2}^T, \hspace{3mm}
    Q_2 = \frac{1}{\sqrt{2}} U_{3 \times 3}\begin{bmatrix} 0 & 1 \\ 1 & 0 \\ 0 & 0\end{bmatrix} V_{2 \times 2}^T, \nonumber \\
    Q_3 = U_{3 \times 3} \begin{bmatrix} 0 & 0 \\ 0 & 0 \\ 1 & 0\end{bmatrix} V_{2 \times 2}^T, \hspace{3mm}
    Q_4 = U_{3 \times 3} \begin{bmatrix} 0 & 0 \\ 0 & 0 \\ 0 & 1\end{bmatrix} V_{2 \times 2}^T. \nonumber \\
    Q_5 = U_{3 \times 3} \begin{bmatrix}
    a_1 & 0 \\ 0 & a_2 \\ 0 & 0
    \end{bmatrix} V_{2 \times 2}^T, \hspace{3mm}
    Q_6 = U_{3 \times 3} \begin{bmatrix}
    b_1 & 0 \\ 0 & b_2 \\ 0 & 0
    \end{bmatrix} V_{2 \times 2}^T. \nonumber
\end{align}
```

```math
\begin{align}
    \lambda_1 = \frac{1}{\sigma_1+\sigma_2}\left( \frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma_1} + \frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma_2}\right), \nonumber \\
    \lambda_2 = \frac{1}{\sigma_1-\sigma_2}\left( \frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma_1} - \frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma_2}\right), \nonumber \\
    \lambda_3 = \frac{1}{\sigma_1}\left(\frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma_1}\right), \hspace{3mm}
    \lambda_4 = \frac{1}{\sigma_2}\left(\frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma_2}\right). \nonumber
\end{align}
```

Note that our SVD call yields $U_{3 \times 2}$, but the ones used above are $U_{3 \times 3}$. You can convert from $U_{3 \times 2}$ to $U_{3 \times 3}$ by extending it with another column. This additional column can be obtained by taking the cross product of the first and second columns of $U_{3 \times 2}$, such that

```
/* Apache v2.0 License, though this is too simple to claim... */
Matrix3x3 extend_U(const Matrix3x2 &U) {
    Vector3 cross = U.col(0).cross(U.col(1));
    Matrix3x3 result;
    result << U.col(0), U.col(1), cross;
    return result;
}
```

All looks good, except we‚Äôre still missing $a_1$, $a_2$, $b_1$, $b_2$, $\lambda_5$, and $\lambda_6$.
Similar to our 3D case, two vectors $[a_1, a_2]$, $[b_1, b_2]$ and their corresponding scalars $\lambda_5$ and $\lambda_6$ are given as the pair of eigenvectors and eigenvalues of the following symmetric $2 \times 2$ matrix:

```math
\begin{equation}
    H_{2 \times 2} = \begin{bmatrix}
    \displaystyle \frac{\partial^2}{\partial \sigma_1^2} & \displaystyle \frac{\partial^2}{\partial \sigma_1 \partial \sigma_2} \\
    \displaystyle \mathrm{Sym} & \displaystyle \frac{\partial^2}{\partial \sigma_2^2}
    \end{bmatrix} \Psi_{\mathrm{2D}}. \nonumber
\end{equation}
```

> [!NOTE]
> When $\Psi_{\mathrm{2D}}$ is written in terms of two Green-Cauchy invariants $I_1 = \mathrm{tr}(F^TF) = \sigma_1^2 + \sigma_2^2$ and $I_2 = \mathrm{det}(F^TF) = \sigma_1^2 \sigma_2^2$ such that:
>
> ```math
> \begin{equation}
>  \Psi_{\mathrm{2D}} = \Psi(I_1,I_2), \nonumber
> \end{equation}
> ```
>
> All singularities can be removed as:
>
> ```math
> \begin{equation}
>   \begin{bmatrix}
>       \lambda_1 \\
>       \lambda_2 \\
>       \lambda_3 \\
>       \lambda_4
>   \end{bmatrix} =
>   2 \begin{bmatrix}
>      1 & \sigma_1 \sigma_2 \\
>      1 & -\sigma_1 \sigma_2 \\
>      1 & \sigma_2^2 \\
>      1 & \sigma_1^2
>   \end{bmatrix}
>   \begin{bmatrix}
>       \frac{\partial}{\partial I_1} \\
>       \frac{\partial}{\partial I_2}
>   \end{bmatrix}
>   \Psi_{\mathrm{2D}}. \nonumber
> \end{equation}
> ```
>
> Similarly to the 3D case, we can leverage the chain rule to obtain $H_{2 \times 2}$ using only the derivatives of $\Psi_{2 \times 2}$ with respect to invariants.
> Derivatives of $I_k$ with respect to singular values are:
>
> ```math
> \begin{align}
> \frac{\partial I_1}{\partial \sigma} = 2 \left[\begin{matrix}\sigma_{1}\\\sigma_{2}\end{matrix}\right],
> \hspace{3mm}
> \frac{\partial I_2}{\partial \sigma} = 2 \left[\begin{matrix}\sigma_{1} \sigma_{2}^{2} + \sigma_{1} \sigma_{3}^{2}\\\sigma_{1}^{2} \sigma_{2} + \sigma_{2} \sigma_{3}^{2}\end{matrix}\right], \nonumber \\
> \hspace{3mm}
> \frac{\partial^2 I_1}{\partial \sigma^2} = 2 \left[\begin{matrix}1 & 0\\0 & 1\end{matrix}\right],
> \hspace{3mm}
> \frac{\partial^2 I_2}{\partial \sigma^2} = 2 \left[\begin{matrix}\sigma_{2}^{2} + \sigma_{3}^{2} & 2 \sigma_{1} \sigma_{2}\\  \mathrm{Sym} & \sigma_{1}^{2} + \sigma_{3}^{2}\end{matrix}\right]. \nonumber
> \end{align}
> ```
>
> The first derivative $\frac{\partial \Psi_{\mathrm{2D}}}{\partial \sigma}$ can be computed similarly to the 3D case. The formula for $\frac{\partial I_k}{\partial F}$ also remains the same.
> Interested readers are referred to our verification Python code [(Code)](../eigsys/eigsys_invariants_2.py).

Everything else is exactly the same as the 3D case presented above.
We provide another Python code [(Code)](../eigsys/eigsys_2.py) to numerically verify this analysis.

## üéì Re-Deriving Smith et al. [(1)](#1)

Our technique can be used to arrive at the same eigen system revealed by Smith et al. [(1)](#1).
We can confirm this by simply swapping the Cauchy-Green invariants with those of Smith et al. [(1)](#1) and substitute them into our eigenvalue expressions.
This is simple enough to do manually, but I‚Äôve written a SymPy code to help facilitate the task:

```
from sympy import *

a, b, c = symbols('\sigma_1 \sigma_2 \sigma_3')

I1, I2, I3 = a + b + c, a**2 + b**2 + c**2, a*b*c
E = Function('\Psi')(I1, I2, I3)

display(ratsimp((E.diff(a) + E.diff(b)) / (a + b)))
display(ratsimp((E.diff(c) + E.diff(a)) / (c + a)))
display(ratsimp((E.diff(b) + E.diff(c)) / (b + c)))
display(ratsimp((E.diff(a) - E.diff(b)) / (a - b)))
display(ratsimp((E.diff(c) - E.diff(a)) / (c - a)))
display(ratsimp((E.diff(b) - E.diff(c)) / (b - c)))
```

Now look at Equations (7.16 to 7.21) from [(B)](#B).
The output is identical.
This can't be a coincidence.

<a id="B">[B]</a> Theodore Kim and David Eberle. 2022. Dynamic deformables: implementation and production practicalities (now with code!). In ACM SIGGRAPH 2022 Courses (SIGGRAPH '22). <https://doi.org/10.1145/3532720.3535628>

We have now proven the equivalence of $Q_{1 \cdots 6}$ and $\lambda_{1 \cdots 6}$.
Looks good, but what about scaling mode eigenmatrices $Q_{7 \cdots 9}$ and $\lambda_{7 \cdots 9}$?
Smith et al. [(1)](#1) define $Q_{7,8,9}$ using the eigenvectors and eigenvalues of an encoded matrix $A$ (see their paper for the full representation).
The eigenvectors and eigenvalues are arranged in exactly the same way as ours.
Therefore, to prove that our $Q_{7,8,9}$ and $\lambda_{7,8,9}$ are exactly the same, it is sufficient to show that $H_{3 \times 3} = A$.
For this purpose, let's again use the following chain rule, but this time with $I_k$ being the invariants from Smith et al. [(1)](#1).

```math
H_{3 \times 3} = \sum_{i,j} \frac{\partial^2 \Psi_{\mathrm{3D}}}{\partial I_i \partial I_j} \left(\frac{\partial I_i}{\partial \sigma}\right)\left(\frac{\partial I_j}{\partial \sigma}\right)^T + \sum_k \left(\frac{\partial \Psi_{\mathrm{3D}}}{\partial I_k}\right) \left(\frac{\partial^2 I_k}{\partial \sigma^2}\right),
```

Doing everything by hand is too much work, so let's use SymPy again.
The following code constructs $H_{3 \times 3}$ and $A$ as defined by Smith et al. [(1)](#1) and compares the differences.

```
from sympy import *

a, b, c = symbols('\sigma_1 \sigma_2 \sigma_3')
sym_I1, sym_I2, sym_I3 = symbols('I_1 I_2 I_3')
E = Function('\Psi')(sym_I1, sym_I2, sym_I3)
I1, I2, I3 = a+b+c, a**2+b**2+c**2, a*b*c
I, sym_I, s = [I1,I2,I3], [sym_I1,sym_I2,sym_I3], [a,b,c]

dEdI = Matrix([E.diff(sym_I[i]) for i in range(3)])
d2EdI2 = Matrix([[E.diff(sym_I[i], sym_I[j])
            for i in range(3)]
            for j in range(3)])
dIds = [Matrix([I[k].diff(s[i]) for i in range(3)])
                                for k in range(3)]
d2Ids2 = [Matrix([[I[k].diff(s[i],s[j])
          for i in range(3)] for j in range(3)]) 
          for k in range(3)]

# Our H_3x3 matrix
H3x3 = zeros(3)
for i in range(3):
    H3x3 += dEdI[i] * d2Ids2[i]
    for j in range(3):
        H3x3 += d2EdI2[i,j] * dIds[i] * dIds[j].T

# Smith et al. (2019)
A = zeros(3)
for i in range(3):
    A[i,i] = 2*dEdI[1]+d2EdI2[0,0]+4*s[i]**2*d2EdI2[1,1] \
         + I3**2/s[i]**2*d2EdI2[2,2]+4*s[i]*d2EdI2[0,1] \
         + 4*I3*d2EdI2[1,2]+2*I3/s[i]*d2EdI2[0,2]
for i in range(3):
    for j in range(3):
       if i < j:
           k = 3 - i - j
           A[i,j] = s[k]*dEdI[2]+d2EdI2[0,0] \
             + 4*I3/s[k]*d2EdI2[1,1] \
             + s[k]*I3*d2EdI2[2,2] \
             + 2*s[k]*(I2-s[k]**2)*d2EdI2[1,2] \
             + (I1-s[k])*(s[k]*d2EdI2[0,2]+2*d2EdI2[0,1])
           A[j,i] = A[i,j]

# Symbolically compute the difference
display(simplify(H3x3-A))
```

This prints zero! üò≤ But if you tweak the way $A$ is computed a little bit, it gives non-zero expressions üî¢, so this must be correct ‚úÖ.
This also confirms that their encoded matrix $A$ corresponds to $\frac{\partial^2 \Psi}{\partial \sigma^2}$.
Now we have proven that our eigen analysis can re-derive the same system as Smith et al. [(1)](#1).

## üç± Takeaway C/C++ ‚öôÔ∏è and Rust ü¶Ä Codes

If you‚Äôre feeling excited, we‚Äôd like to share full C/C++ and Rust implementations of the two analyses above under the **Apache v2.0 license**. üìú
Here they are [(üìÇ Mini Project Directory)](../eigsys).
This also runs on CUDA with minor adaptation work.

